# Study id
nmdc:sty-11-547rwq94

# Configurations
Configurations generated guided by documentation here: https://github.com/biocore/emp/blob/master/protocols/MetabolomicsLC.md and looking at the instrument/running details that are the output of the `scripts/raw_file_info_pull_logger.py` script (see Mapping section below).

Configurations generated by _emp_500_metabolomics/config_generation.py

# Raw data
Massive id = MSV000083475
ftp link = ftp://massive-ftp.ucsd.edu/v02/MSV000083475/raw/RAW

Local copy here (note no subfolders, all in one folder, unlike the ftp):
- /Users/heal742/Library/CloudStorage/OneDrive-PNNL/Documents/_DMS_data/_NMDC/_massive/_emp500_lcms/RAW/to_process

# Processed data
Json prepper script: _emp_500_metabolomics/scripts/wdl_json_preppper.py.

This prepares the json files (_emp_500_metabolomics/wdl_jsons) in batches of 50 files each to minimize the number of raw files we need to have locally loaded at one time.

Shell script to run the WDLs in batches (one json per run): _emp_500_metabolomics/scripts/emp500_metab.sh.
To run
```bash
chmod +x _emp_500_metabolomics/scripts/emp500_metab.sh

# Run the script
/Users/heal742/LOCAL/05_NMDC/02_MetaMS/data_processing/_emp_500_lcms_metabolomics/scripts/emp500_metab.sh
```

To move the processed data from the wdl locations to another place, use the `processed_data_mover.py` script. Then if there are remaining files, rerun the json prepper script to generate new json files for the remaining raw files.

# Mapping
1. Pull ncbi information from nmdc biosamples using `scripts/biosample_ncbi_mapper.py` script (writes `_emp_500_lcms_metabolomics/biosample_attributes.csv`)
2. Get start and end times, instrument details using `scripts/raw_file_info_pull_logger.py` script (writes `_emp_500_lcms_metabolomics/raw_file_info_TIMESTAMP.csv` and `_emp_500_lcms_metabolomics/processing_errors_TIMESTAMP.csv`)
**One problematic file was found: `1E11_2_27_bowen-74-s010-a04.raw`
3. Get ftp locations for the MASSIVE study by calling the following in the terminal (this will take a while and writes `_emp_500_lcms_metabolomics/emp500_massive_ftp_locs.txt`):
```bash
wget --spider -r -nd ftp://massive-ftp.ucsd.edu/v02/MSV000083475/raw/RAW -o _emp_500_lcms_metabolomics/emp500_massive_ftp_locs.txt
```
4. Clean the mapping using the `scripts/metadata_map_maker.py` to get file name to biosample id (writes `_emp_500_lcms_metabolomics/mapped_raw_data_files.csv`)

# Notes
On 7/18/25, we had 612 samples processed and 56 problematic files. Two main issues were found - a memory issue and a divide by zero error.  The memory issue was somewhat resolved by increasing the available memory for docker, but not completely.