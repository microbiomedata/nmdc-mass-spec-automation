# Study id
nmdc:sty-11-547rwq94

# Configurations
Configurations generated guided by documentation here: https://github.com/biocore/emp/blob/master/protocols/MetabolomicsLC.md and looking at the instrument/running details that are the output of the `scripts/raw_file_info_pull_logger.py` script (see Mapping section below).

Configurations generated by _emp_500_metabolomics/config_generation.py

# Raw data
Massive id = MSV000083475
ftp link = ftp://massive-ftp.ucsd.edu/v02/MSV000083475/raw/RAW

Local copy here (note no subfolders, all in one folder, unlike the ftp):
- /Users/heal742/Library/CloudStorage/OneDrive-PNNL/Documents/_DMS_data/_NMDC/_massive/_emp500_lcms/RAW/to_process

# Processed data
Json prepper script: _emp_500_metabolomics/scripts/wdl_json_preppper.py.

This prepares the json files (_emp_500_metabolomics/wdl_jsons) in batches of 50 files each to minimize the number of raw files we need to have locally loaded at one time.

Shell script to run the WDLs in batches (one json per run): _emp_500_metabolomics/scripts/emp500_metab.sh.
To run
```bash
chmod +x _emp_500_metabolomics/scripts/emp500_metab.sh

# Run the script
/Users/heal742/LOCAL/05_NMDC/02_MetaMS/data_processing/_emp_500_lcms_metabolomics/scripts/emp500_metab.sh
```

To move the processed data from the wdl locations to another place, use the `processed_data_mover.py` script. Then if there are remaining files, rerun the json prepper script to generate new json files for the remaining raw files.

# Mapping
1. Pull ncbi information from nmdc biosamples using `scripts/biosample_ncbi_mapper.py` script (writes `_emp_500_lcms_metabolomics/biosample_attributes.csv`)
2. Get start and end times, instrument details using `scripts/raw_file_info_pull_logger.py` script (writes `_emp_500_lcms_metabolomics/raw_file_info_TIMESTAMP.csv` and `_emp_500_lcms_metabolomics/processing_errors_TIMESTAMP.csv`)
**One problematic file was found: `1E11_2_27_bowen-74-s010-a04.raw`
3. Get ftp locations for the MASSIVE study by calling the following in the terminal (this will take a while and writes `_emp_500_lcms_metabolomics/emp500_massive_ftp_locs.txt`):
```bash
wget --spider -r -nd ftp://massive-ftp.ucsd.edu/v02/MSV000083475/raw/RAW -o _emp_500_lcms_metabolomics/emp500_massive_ftp_locs.txt
```
4. Clean the mapping using the `scripts/metadata_map_maker.py` to get file name to biosample id (writes `_emp_500_lcms_metabolomics/mapped_raw_data_files.csv`)

# Notes
As of 7/21/25, we have 617 samples processed!

## Processing issues
Two main issues were found - a memory issue and a KeyError during the deconvolutions step.  The memory issue was somewhat resolved by increasing the available memory for docker, but not completely. So far only 1 gridding error.

- 4E8_1_68_thomas-18-s057-a03.raw - gridding failed during peak picking. _FIXED!_
- 4C1_3_60_pinto-63-s003-a04.raw - unclear why this failed, no error message logged.

### Using branch of corems called 219_decon_checks, trying to process again with more debugging print messages to understand where files failed.
The following samples failed during find mass features stage with no error messages 
- 4C1_3_60_pinto-63-s003-a04.raw (Up to 10 G memory being used during the find mass features step wiht no metadata loaded)
- 4C7_3_80_pinto-63-s009-a04.raw
- 4A1_2_4_pinto-63-s024-a04.raw
This looks like it's related to ripser underneath and we'll need to partition this somehow.  We could break up in time chunks?

Trying to save and pickle the metadata to free up memory during the ms1 data processing


#
f.MSV000083475/raw/RAW/PLATE7/Plate7.sld <- no https: available>

https://massive.ucsd.edu/ProteoSAFe/DownloadResultFile?file=f.MSV000090886%2Fccms_peak%2Fmzml_files%2F20220211_JGI_MD_507130_BioS_final1_IDX_C18_USDAY59443_NEG_MSMS_0_QC_Post_Rg80to1200-CE102040--QC_Run1078.mzML&forceDownload=true

https://massive.ucsd.edu/ProteoSAFe/DownloadResultFile?file=f.MSV000083475%2Fraw%2FRAW%2FPLATE3and4%2FWash_01_20190212070927.raw&forceDownload=true