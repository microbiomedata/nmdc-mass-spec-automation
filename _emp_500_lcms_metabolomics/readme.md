# Study id
nmdc:sty-11-547rwq94

# Configurations
Configurations generated guided by documentation here: https://github.com/biocore/emp/blob/master/protocols/MetabolomicsLC.md

Configurations generated by _emp_500_metabolomics/config_generation.py

# Raw data
Local copy here:
- /Users/heal742/Library/CloudStorage/OneDrive-PNNL/Documents/_DMS_data/_NMDC/_massive/_emp500_lcms/RAW/to_process

Massive id = MSV000083475
ftp link = ftp://massive-ftp.ucsd.edu/v02/MSV000083475/raw/RAW

# Processed data
Json prepper script: _emp_500_metabolomics/scripts/wdl_json_preppper.py.

This prepares the json files in batches of 50 files each to minimize the number of raw files we need to have locally loaded at one time.

Shell script to run the WDLs in batches: _emp_500_metabolomics/scripts/emp500_metab.sh.
To run
```bash
chmod +x _emp_500_metabolomics/scripts/emp500_metab.sh

# Run the script
/Users/heal742/LOCAL/05_NMDC/02_MetaMS/data_processing/_emp_500_lcms_metabolomics/scripts/emp500_metab.sh
```

This script will run the WDL in batches of 50 files each.

# Mapping
1. Pull ncbi information from nmdc biosamples using `scripts/biosample_ncbi_mapper.py` script (writes `_emp_500_lcms_metabolomics/biosample_attributes.csv`)
2. Clean the mapping using the `scripts/metadata_map_maker.py` to get file name to biosample id (writes `_emp_500_lcms_metabolomics/mapped_raw_data_files.csv`)
3. Get start and end times, instrument details using `scripts/raw_file_info_pull_logger.py` script (writes `_emp_500_lcms_metabolomics/raw_file_info_TIMESTAMP.csv` and `_emp_500_lcms_metabolomics/processing_errors_TIMESTAMP.csv`)
**One problematic file was found: `1E11_2_27_bowen-74-s010-a04.raw`
#TODO KRH: need to get ftp file locations for the url

Minio locations (in metabolomics bucket):
- 

MASSIVE ftp locations:
```
wget --spider -r -nd ftp://massive-ftp.ucsd.edu/v02/MSV000083475/raw/RAW -o emp500_massive_raw_v2.txt
grep -oP 'ftp://[^ ]+\.raw' emp500_massive_raw_v2.txt > emp500_massive_raw_v2_clean.txt
```