# Study id
nmdc:sty-11-r2h77870

_Note that we are only processing the rhizosphere samples which have "rhizo", "leaf", or "root" in the name_ (roughly 1350 MS runs).

# Raw data
Massive id = MSV000090886
ftp link = ftp://massive-ftp.ucsd.edu/v05/MSV000090886/raw

First pull the ftp locations using the following command:
```bash
wget --spider -r -nd ftp://massive-ftp.ucsd.edu/v05/MSV000090886/raw -o _bioscales_lcms_metabolomics/bioscales_massive_ftp_locs.txt
```

Next download the raw files using the `_bioscales_lcms_metabolomics/scripts/file_puller.py`.
This will download files to `/Users/heal742/Library/CloudStorage/OneDrive-PNNL/Documents/_DMS_data/_NMDC/_massive/_bioscales_lcms/to_process` and write a file `_bioscales_lcms_metabolomics/bioscales_ftp_locs.csv` with the ftp locations of the files that were downloaded and the raw file name.  Note that this will only download files that are tagged with "rhizo" in the name.

# Configurations _WIP_
Configurations generated guided by documentation here: https://www.nature.com/articles/s41597-024-03069-7 and looking at the instrument/running details that are the output of the `scripts/raw_file_info_pull_logger.py` script (see Mapping section below).

There will be two chromatogram configurations - one for HILICZ and one for C18.

There will be at least two mass spectrometer configurations - one for the positive ion mode and one for the negative ion mode.
There may be different mass spectrometer configurations for the different chromatogram configurations, but this is not yet confirmed.

There are two mass spec configurations for each chromat run - one for collision energy 102040 and another for 205060

Configurations generated by `_bioscales_lcms_metabolomics_/config_generation.py`

# Processed data _WIP_
Json prepper script: _emp_500_metabolomics/scripts/wdl_json_preppper.py.

This prepares the json files (_emp_500_metabolomics/wdl_jsons) in batches of 50 files each to minimize the number of raw files we need to have locally loaded at one time.

Shell script to run the WDLs in batches (one json per run): _emp_500_metabolomics/scripts/emp500_metab.sh.
To run
```bash
chmod +x _emp_500_metabolomics/scripts/emp500_metab.sh

# Run the script
/Users/heal742/LOCAL/05_NMDC/02_MetaMS/data_processing/_emp_500_lcms_metabolomics/scripts/emp500_metab.sh
```

# Mapping _WIP_
1. Pull ncbi information from nmdc biosamples using `scripts/biosample_ncbi_mapper.py` script (writes `_emp_500_lcms_metabolomics/biosample_attributes.csv`)
2. Get start and end times, instrument details using `scripts/raw_file_info_pull_logger.py` script (writes `_emp_500_lcms_metabolomics/raw_file_info_TIMESTAMP.csv` and `_emp_500_lcms_metabolomics/processing_errors_TIMESTAMP.csv`)
**One problematic file was found: `1E11_2_27_bowen-74-s010-a04.raw`
3. Add ftp locations for the MASSIVE study by calling the following in the terminal (this will take a while and writes `_emp_500_lcms_metabolomics/emp500_massive_ftp_locs.txt`):
4. Clean the mapping using the `scripts/metadata_map_maker.py` to get file name to biosample id (writes `_emp_500_lcms_metabolomics/mapped_raw_data_files.csv`)



#TODO KRH: need to get ftp file locations for the url

Minio locations (in metabolomics bucket):
- 

```
grep -oP 'ftp://[^ ]+\.raw' emp500_massive_raw_v2.txt > emp500_massive_raw_v2_clean.txt
```